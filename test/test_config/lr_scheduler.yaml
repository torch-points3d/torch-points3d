training:
    epochs: 500
    num_workers: 4
    batch_size: 8
    shuffle: True
    cuda: -1
    precompute_multi_scale: False # Compute multiscate features on cpu for faster training / inference
    optim:
        base_lr: 0.1
        # accumulated_gradient: -1 # Accumulate gradient accumulated_gradient * batch_size
        grad_clip: -1
        optimizer:
            class: SGD
            params:
                lr: ${training.optim.base_lr} # The path is cut from training
                momentum: 0.9
                dampening: 0.1
                weight_decay: 1
        lr_scheduler:
            class: StepLR
            params:
                on_epoch:
                    step_size: 1
                    gamma: 0.9
                    last_epoch: -1
                on_num_sample:
                    step_size: 32
                    gamma: 0.9
                    last_epoch: -1
                on_num_batch:
                    step_size: 1
                    gamma: 0.9
                    last_epoch: -1
        bn_scheduler:
            bn_policy: "step_decay"
            params:
                bn_momentum: 0.02
                bn_decay: 1
                decay_step : 1000
                bn_clip : 1e-2
    weight_name: "latest" # Used during resume, select with model to load from [miou, macc, acc..., latest]
    enable_cudnn: True
    checkpoint_dir: ""
update_lr_scheduler_on: "on_epoch" # ["on_epoch", "on_num_batch", "on_num_sample"]
