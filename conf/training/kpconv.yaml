# Those arguments defines the training hyper-parameters
training:
    shuffle: True
    num_workers: 6
    batch_size: 16
    cuda: 1
    precompute_multi_scale: True # Compute multiscate features on cpu for faster training / inference
    epochs: 100
    grad_clip: -1
    optimizer:
        class: Adam
        params:
            lr: 0.001
    learning_rate:
        scheduler_type: "exponential_decay"
        base_lr: ${optimizer.params.lr}
        gamma: 0.9716 # Divide learning by 10 every 80 epochs
        decay_step: 14016
        lr_clip: 1e-5
    weight_name: "latest" # Used during resume, select with model to load from [miou, macc, acc..., latest]
    enable_cudnn: True
    checkpoint_dir: ""
    resume: True

# Those arguments within experiment defines which model, dataset and task to be created for benchmarking
# parameters for Weights and Biases
wandb:
    project: shapenet-pn2
    log: True

# parameters for TensorBoard Visualization
tensorboard:
    log: True
