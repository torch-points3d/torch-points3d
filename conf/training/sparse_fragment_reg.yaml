# Those arguments defines the training hyper-parameters
# @package training
epochs: 200
num_workers: 6
batch_size: 4
shuffle: True
cuda: 0
precompute_multi_scale: False # Compute multiscate features on cpu for faster training / inference
optim:
  base_lr: 1e-1
  # accumulated_gradient: 1
  # accumulated_gradient: -1 # Accumulate gradient accumulated_gradient * batch_size
  grad_clip: -1
  optimizer:
    class: SGD
    params:
      lr: ${training.optim.base_lr} # The path is cut from training
      momentum: 0.8
      weight_decay: 1e-4
  lr_scheduler:
    class: ExponentialLR
    params:
      gamma: 0.99
  # bn_scheduler:
  #    bn_policy: "step_decay"
  #    params:
  #        bn_momentum: 0.1
  #        bn_decay: 0.9
  #        decay_step : 3000
  #        bn_clip : 1e-2
weight_name: "latest" # Used during resume, select with model to load from [miou, macc, acc..., latest]
enable_cudnn: True
checkpoint_dir: ""

# Those arguments within experiment defines which model, dataset and task to be created for benchmarking
# parameters for Weights and Biases
wandb:
  project: registration
  log: True
  notes:
  name: humanpose1
  public: True # It will be display the model within wandb log, else not.

  # parameters for TensorBoard Visualization
tensorboard:
  log: True
